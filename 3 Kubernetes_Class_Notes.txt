# 05 Feb Wed - Kubernetes -1 Explanation & Vinala Repeat Required
# 07 Feb Fri - Kubernetes -2 Architecture
# 10 Feb Mon - Kubernetes -3 Master Node Explanation
# 12 Feb Wed - Kubernetes -4 Pod Working & Architecture
# 14 Feb Wed - Kubernetes -5 Recording Not Avaiable in PC  && pending
# 17 Feb Mon - Kubernetes -6 Flannel & crictl & Installation 
# 19 Feb Wed - Kubernetes -7 Label
# 21 Feb Fri - Kubernetes -8 Namespace
# 24 Feb Mon - Kubernetes -9 ReplicaSet
# 26 Feb Wed - Kubernetes -10 Deployment
# 28 Feb Fri - Kubernetes -11 Service
# 03 Mar Mon - Kubernetes -12 Service Pratical & Helm Explain
# 05 Mar Wed - Kubernetes -13 Helm Pratical

================================================================================================================================================================
## Kubernetes Interview Questions

1) What is the difference between a stateless and stateful pod or container in Kubernetes?
2) What are deployments in Kubernetes? Explain the different types of deployments.
3) What is a ReplicaSet, and how does it differ from a ReplicationController?
4) Explain the architecture and workflow of Kubernetes.
5) What is the difference between vertical scaling and horizontal scaling in a Kubernetes pod?
G) How does autoscaling work in Kubernetes? Explain the Horizontal Ped Autoscaler (HPA) and Vertical Pod Autoscaler (VPA).
7) How does resource allocation work in Kubernetes? Explain CPJ and memory limits for a pod.
8) What is an Ingress Controller in Kubernetes, and how does it manage external traffic?
9) What is a Service in Kubernetes? Explain different types of services such as ClusterIP, NodePort, and LoadBalancer.
10) How does Kubernetes handle service discovery and load balancing?
11) How do you take a backup of the etd database in Kubernetes?
12) What is a Persistent Volune (PV) and a Persistent Volume Claim (PVC)?
13) Explain the difference between emptyDir, hostPath, and Persistent Volumes.
14) How do you retrieve logs from a Kubernetes application pod?
15) What are self-healing capabilities in Kubernetes, and how dces Kubernetes ensure high availability?
16) What are the common troubleshooting steps for failed pods?
17) How do you monitor Kubernetes clusters using Prometheus and Grafana?
18) What is the difference betwsen kubectl create and kubectl apply?
19) What are ConfigMaps and Secrets, and how do they help manage configurations in Kubernetes?
20) What is a Heln chart, and how does it simplify Kubernetes deployments?
21) How does Kubernetes handle RBAC (Role-Based Access Control)?
22) What are Network Policies, and how do they secure pod conmunication?
23) How can you secure container inages in Kubernetes deploynents?
================================================================================================================================================================

# 05 Feb Wed - Kubernetes -1

Kubernetes 

Required 2 Machines 
1. Control Plane / Master 
2. Worker 
And Swap Memory should be OFF it's is mandatory.
And required Network drive rule.And Kernel module.

Then install perquisites and install Kubernetes.
Vinala set-up.

kubectl create deployment nginx --image=nginx
kubectl get pod   --> To see running Pods

kubectl expose deployment nginx --type=NodePort --port=80
kubectl get svc nginx 

Search with port which kub bydefault open for Nginx webpage & Open Port in worker node
================================================================================================================================================================

================================================================================================================================================================

================================================================================================================================================================
# 07 Feb Fri - Kubernetes -2 Architecture

# Borg is old name of K8s when google is using.
Kubernetes is workstation / Orchestration.

### Architecture ###
Admin to Kube-API-Server then Controller Manger then asking take details form etcd-databse then Controller Manger give to Kube scheduler then go to again API then go to the node server then create containerd.

Path:
"C:\Users\Dell\Desktop\IPM_CMDS\SS\SS_9__07-02-25_Kube_Arch.png"
"C:\Users\Dell\Desktop\IPM_CMDS\SS\SS_10__07-02-25_Kube_Arch.png"

container management docker 
docker -- swarm -- conatiner --
docker product -- open source 

package -- install -- cust --
cust
cust --- open swarm -- highload , security , scaling , load balancer

container -- swarm --


kublet 
kube - Manger
kube - Schduler
kube - Proxy
etcd - databse 
api - server
Container engine:
 = Container CRI
 - containerd
 - Rocket
 - Podman

## Install ##
Update the systen and install dependencies:

sude apt update && sudo apt upgrade -y
sudo apt install apt-transport-https curl -y

Install containerd:

sudo apt install containerd -y

Configure containerd:

sudo nkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml > /dev/null
sudo sed -i 's/SystendCgroup - false/SystemdCgroup - true/' /etc/containerd/config.toml
sudo systenctl restart containerd

Disable swap:

sudo swapoff -a
sudo sed -i '/ swap / s/^\( .* \)S/#\1/g' /etc/fstab

Load necessary kernel modules:

sudo nodprobe overlay
sudo nodprobe br_netfilter

Set required sysctl parameters:

cat << EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables - 1
net.ipv4.ip_forward
EOF
sude sysctl -- system
.

# Commands
kubctl get node
systemctl status conatinerd.service
systemctl status kubelet.service
kubectl --help
kubectl get pod -A  # To see all running pods.

KUBECONFIG=$HOME/.kube/c
echo $HOME/.kube/config/root/.kube/config

kubectl delete pod  
kubectl delete pod nginx-bf5d5cf98-db4kd  # To delete pod
kubectl run pod nginx
kubectl get pod

================================================================================================================================================================

================================================================================================================================================================

# 10 Feb Mon - Kubernetes -3 Master Node Explanation

Architecture:
 Admin to Kube-API-Server then Controller Manger then asking take details form etcd-databse then Controller Manger give to Kube scheduler then go to again API then go to the node server then create containerd.
-------------------------------------------------

etcd like zookeeper database also present.

Maximum we can use two master nodes.

kubectl get node
kubectl get pod -A.  # We can see all master nodes content 
 Like:
   - coredns
   - etcd
   - kube-api-server
   - kube-controller-manager-ip
   - kube-porxy-x8bhm
   - kube-scheduler

# For accessing master node via other Linux & Windows.
  Download kubectl and configure in environment variables.
  And create folder and paste admin content in config file. For this search on google.


## To get access in normal Linux doesn't not required k8 nodes. 
mkdir  .kube
cd    .kube
ls
vi config    #  Paste from Master  etc/kubernetes/admin.conf To get access. 
cat /etc/kubernetes/admin.conf   #  In master Node.

================================================================================================================================================================

================================================================================================================================================================

# 12 Feb Wed - Kubernetes -4 Pod Working & Architecture

# Compenets name:
Eg : Docker swarm (Docker), Kubernetes (Google), mesos (Apache). 

kubectl:
 - kubectl run hello-minikube
 - kubectl cluster-info
 - kubectl get nodes
# Don't tell we use minikube, because it's present like your are learning kubernetes. 

## There are three types of service:
Service:
    - clusterip
    - Nodeport
    - Loadbalancer


## Main Working  ##
Service IP ---> Deployment ---> Pod ---> Pause ---> Container.

- Whenever Pod creates its create one Pause conatiner also, means that no.of craeted container plus one pause container also (eg: Nginx + Pause). If container delete then Pause its use to recreate that containers.
- Whenever Pod gets delete deleted then Deployment its use for recreate that Pod.


Pod means:
 - Logical boundaries and collection of containers.
 - In Pod automatically creates one Pause container for storing all details of pod and conatiner and pause doesn't have personal IP. 

Pause means:
 - Its storing all details of pod and conatiner and pause doesn't have personal IP. And its use for recreate that container.

kubeadm token create --print-join-command    # To Print command of join in master, so in worker we paste that cmd to joining node. 
kubectl get node   #  To see no.of nodes. 
kubectl get pod  #  To see no.of pods.

kubectl run db-server --image=mysql   # To create Pod.
kubctl run webserver --image=nginx.
kubectl get pod  #  To see no.of pods. 

kubectl exec -it webserver /bin/bash   #  To login in container same like command in docker. 

## For Copy and paste in kubernetes play in browser:
Copy ctrl+insert+c
Paste ctrl+insert+v

================================================================================================================================================================

# 14 Feb Fri 25 - Recording Not Avaiable in PC.
Topic : K8s Cluster Reset and Token Management 
kubeadm, etcd --> IQ 

Kubeadm reset: Phases - 
1. preflight 
2. update-cluster-status 
3. remove-etcd-member
4. cleanup-nodes 

kubeadm reset --preflight
kubeadm reset --skip-phases remove-etcd-member    #To remove only etcd. 

# Launch two instnace for Master & Node then install Kubernetes
 
systemctl status containerd.service
watch kubectl get node -A    # To see Ip Also In master 
cd /var/lib/etcd/member/snap # To see database 
cd /etc/kubernetes/manifests # To see Configuration
cd /etc/kubernetes/pki     # Same above


kubectl reset   # It's delete all files in /etc/kubernetes and /var/lib, it doesn't not reset work node and other things, it's required manually, rm -rf /etc/cni/net.d file aslo in both master and worker.
systemctl restart containerd.service

## In Worker Node server
kubeadm reset 


In master 
kubeadm init command........
kubectl get pod -A
systemctl restart containerd.service and check status 
kubectl apply -f ......
kubeadm join command.................................
kubectl get pod -A    # To check status of workers nodes connected or not.
# If its show not connected, then go to worker node server and restart the containerd.service and kublet then check again.

# To delete node from cluster
kubectl get pod -A
kubectl delete node worker

kubectl describe pod <kube-name> -n kube-system  # To see IP

## Token Management  ##

#In Master Server 
kubectl get csr
kubeadm token list   # To see list.
kubeadm token create  # To create new Token.
kubeadm token create --ttl=5m   # For only 5mins.
kubeadm token create --ttl=0    # For infinity and it's not delete.
opensllll..... Command 
kubeadm token create --print-join-command   # To join

# In Worker Node
Steps: For Reset is Mandatory.
1. kubectl reset -f
2. rm -rf /etc/cni/net.d
3. systemctl restart containerd.service
4. systemctl restart kublet.service
## rm -rf /etc/kubernetes/pki/ca.crt  --> for only remove CA certificate 
kubeadm join IP -token --discovery......

Step 2: With confirmation Token.
First reset all 
kubectl delete clusterrolebinding kubeadm:node-autoapprove................

Now fire join command in worker node
Its asking approval from Master.

In master 
kubectl get csr
kubectl certificate approve <NAME>    # To approve 

Completed but not have proper commands.
================================================================================================================================================================

================================================================================================================================================================

# 17 Feb Mon - Kubernetes -6 Flannel & crictl

## flannel & crictl search on google ??

# Taint
Means it's just like a flag, If you disabled on master node, then we can create on Pod on master node also. And automatically creating on Pod and other node. Same for all others nodes aslo.

# Kubernetes Installation in Ubuntu

sudo apt install apt-transport-https curl -y
Install containerd:
sudo apt install containerd -y

Canfigure containerd:
sudo mkdir -p /etc/containerd
cantainerd config default | sudo tee /etc/containerd/config.toml > /dev/null
sudo sed -1 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
sudo systenctl restart containerd


Disable swap:
sudo swapoff -a
sudo sed -i '/ swap / s/^\(."\)$/#\1/g' /etc/fstab

Load necessary kernel modules:

sudo modprobe overlay
sudo modprobe br_netfilter

Set required sysctl parameters:

cat << EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
net.ipv4.ip_forward

Install Kubernetes components:
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg -- dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

---------------------------------- Till this we have to run all node that in cluster ---------------------------------------------------


---------------------------------- After this we have to run on master node ( control-plan) ---------------------------------------------------

Initialize the cluster (run only on master node):

sudo kubeadm init -- pod-network-cidr=10.244.0.0/16 ( don't change this ip ) ---- this is for flannel netwrok plugin ### ( run on master node only )
10.....
Set up kubeconfig for the user:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
KUBECONFIG=$HOME/.kube/config

Path : https://kubernetes.io/docs/rehttps://kubernetes.io/docs/retools/kubeadm/kubeadm-join/  ## ??
Install Flannel metwork plugin (run only on master node):
wget https://github.con/flannel-ic/flannel/releases/latest/dowlcad/kube-fla.cel.yml
kubect1 apply f kube flannel.yml

Verify the installation:
kubectl get nodes
kubectl gut pads -- all-namespace



------------------------------
# In Worker Node
Steps: For Reset is Mandatory.
1. kubectl reset -f
2. rm -rf /etc/cni/net.d
3. systemctl restart containerd.service
4. systemctl restart kublet.service
## rm -rf /etc/kubernetes/pki/ca.crt  --> for only remove CA certificate 
kubeadm join IP -token --discovery......

kubeadm join 10.0.0.0:6443 -- token Syt9ng.5jwl2njnsqk1q8k9 -- discovery-token-unsafe-skip-ca-verification
kubeadm .... join

1) anyjoin ----
2) expire -- 24h --- default -- change
3) Autcmatic approve
----------------------------------

kubctl get node
export KUBECONFIG=/etc/kubernetes/admin.conf
kubctl get node

## Pod Deployment ##
1) Command method
2) File method --- vvimp

kubectl get node
kubectl describe node ip-10.0.0.0  # Ip is control-plane.
kubctl get po
kubctl get po -o wide

## crictl ##
Path: https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/
How to list Container in worker or master nodes
1) create /etc/crictl.yaml file
2) restart the containerd service

crictl ps 

kubectl get node
kubectl edit node ip-172-31-37-207

# Taint #
kubectl describe node ip-10.0.0.0 | grep -i taint
kubectl taint nodes ip-10.0.0.0 node-role.kubernetes.io/control-plane:NoSchedule-   # To untaint.
kubectl taint nodes ip-10.0.0.0 node-role.kubernetes.io/control-plane:NoSchedule    # To again taint.

kubctl explain pod

metadata # means data of data =info

kubectl run web-server -- image=nginx -o yaml -- dry-run| > web.yaml
kubectl run web-server -- image=nginx -o yaml > web.yaml
kubectl apply -f web.yaml
kubectl exec -it web-server -- /bin/bash

================================================================================================================================================================


================================================================================================================================================================
# 19 Feb Wed - Kubernetes -7 - Label

kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

# Label in k8s

kubectl get pod --show-label

team=dev
key=value


## In both worker and master

1) kubeadm reset
2) rm -rf /etc/cni/net.d
3) systemctl restart containerd.service
4) systemctl restart kubelet.service
5) rm -rf /etc/kubernetes/pki/ca.crt
6) kubeadm init -- pod-network-cidr=10.244.0.0/16
sudo modprobe br_netfilter

Set required sysctl parameters:
cat << EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF 
sudo sysctl -- system


7) apply flannel network again  --> In master

# worker ---
 - kubectl get node <node-name>
 - kubectl delete node <node-name>

kubectl get pod -A
kubectl delete pod coredns-55cb58b774-fgzmh -n kube-system  # To delete specfic pod.

kubectl delete pod | -n kube-system
kubectl delete pod coredns -* -n kube-system

kubectl logs kube-flannel-ds-rqwdb -n kube-flannel
kubect1 get po -- show-labels

kubectl label pod web run-
kubectl label pod web team=dev
kubectl label pod web env=prod

# Difference Between:
Replication Contoller - RC
Replication Set       - RS

RC -
1) At a time single label mon
2) Rolling update support
3) rollout not support

RS -
1) At a time more then one label
2) Rolling update not supported
3) N/A

Deployment -
1) Multiple label
2) Rolling update
3) Rollout support

# For Difference Between
Path: https://www.mirantis.com/blog/kubernetes-replication-controller-replica-set-and-deployments-understandingreplication-options/

================================================================================================================================================================


================================================================================================================================================================
# 21 Feb Fri - Kubernetes -8 - Namespace

# Namespace:
It created isloate boundaries, so can access specfic user or groups, which one who give access of namesapce.

kubectl get namespace
kubectl creat namespace <your-namespace>
kubectl run web --image=pod -n <your-namespace>

kubectl get po
kubctl get po -A
O/P:

root@master :~ # kubectl get po -A
NAMESPACE        NAME
default          web
kube-flannel
kube-flannel
kube-system
kube-system
kube-system
kube-system
kube-system
kube-system
kube-system
kube-system

NAME
web
kube-flannel-ds-j5hk8
kube-flannel-ds-r7nx4
coredns-55cb58b774-n9jvc
coredns-55cb58b774-rrjph
etcd-master
kube-apiserver-master
kube-controller-manager-master
kube-proxy-j5xkz
kube-proxy-nqlph
kube-scheduler-master
----------

kubectl create namespace test-namespace  # To create namesapce
kubectl run web --image=nginx -n test-namespace
kubectl get po

kubectl get po -n namespace
kubectl get po -n kube-system
kubectl logs coredns-55cb58b774-n9jvc -n kube-system   # To see logs.


# Search on Internet and find how tp use this command and install.
kubectl top
kubectl top po 
crict ps  ## ??
ctr -n k8s.io c ls

kubectl run frontend -image=nginx
kubectl describe po frontend  # To see explaination of that specfic pod.

# label nodeselctor
usecase 1 : worker ---- pod
usecase 2 : namespace -- pod
usecase 3 : dev --- pod --
usecase 4 : patch --- woker --- label
usecase 5 : access
usecase 6 : pod-controller --- rc , rs , deployment ---- label


kubectl get pod --show-labels
kubectl label po webserver run-
kubectl get pod -- show-labels
kubectl label pod webserver team=dev
kubectl get pod -- show-labels

kubectl explain rc
kubectl explain rc -recursive  # Indetails explaination
kubectl run dev -- image=nginx -replica=3

vim rc.yaml
kubectl apply -f rc.yaml

apiVersion: v1
kind: ReplicationController
metadata:
  name: sadhix
spec:
 replicas: 3
 selector:
   team: dev
 template:
   metadata:
     name: sadhix
     labels:
       team: dev
     spec:
       containers:
         - name: sadhix
           image: nginx:1.9.1

## Google : seting a line indent in vim ?
- To indent the current line, or a visual block: ctrl-t, ctrl-d - indent current line forward, backwards (insert mode) visual > or < - indent block by sw (repeat with. ) then try hitting the F5 key while in insert mode (or just :set paste ).




================================================================================================================================================================





================================================================================================================================================================
# 24 Feb Mon - Kubernetes -9 - ReplicaSet

Key differences:

Rolling update functionality:
Deployments have a built-in rolling update mechanism, allowing you to smoothly update your application by replacing pods one at a time with minimal downtime. whereas RCs lack this feature and require manual intervention for rolling updates.

Declarative nature:
Deployments are declarative, meaning you specify the desired state of your application, and Kubernetes automatically manages the transition to that state, while RCs are more imperative, requiring you to explicitly manage updates.

Rollback support:
Deployments can easily roll back to a previous version by leveraging previous ReplicaSets created during updates, whereas RCs do not have this capability.

Management complexity:
Deployments are considered a higher-level abstraction, simplifying the process of managing application updates, while RCs require more manual configuration for rolling updates.

In summary, if you need to perform rolling updates in Kubernetes, use Deployments as they provide a robust and automated way to manage application deployments with features like rollback and fine-grained control over the update process.


kubectl explain rc --recurisive
kubectl run web -- image=web --dry-run=client

kubectl run web -- image=web --dry-run=client -o yaml

kubctl get po
kubectl run -it os --image=ubuntu /bin/bash  # Required all ports are opens in security group in AWS.
kubectl run -it os /bin/bash

kubectl exec os1 -- ls  # To see output without login in conatiner.
kubectl exec os1 -- echo "hello"
kubectl delete -f rc.yaml
kubectl rollout dev=team -- image=nginx:1.9.1   ## ??

vim rs.yaml
-
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: webapp
  labels:
    team: sadhix
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      team: dev
     # team: dev2
      group: qa
	  tier: frontend
  template:
    metadata:
      labels:
      team: dev
     # team: dev2     ## If you give same key name then it takes only latest key and last mentioned.
      group: qa
	  tier: frontend  ## If you give all differnt key name then it takes all.
    spec:
      containers:
      - name: web
        image: nginx
		
.
:vsplit file1 file 2  # To split two files.

kubctl get po,rs




================================================================================================================================================================




================================================================================================================================================================

# 26 Feb Wed - Kubernetes -10 - Deployment

# There are three steps of Rolling & Rollback in Deployment.
1. By editing version in yaml file.
2. And using rollout history
3. kubectl edit deployment <name>.


# In RS
- When Rolling time, its creates new pods and deleting old pods, and does not create service and deleted old pods and not recoveryable. This is the disavdancedment. So no down time required.

# In Deployment
- When updating the new pods, then it create first service and then create one-by-one pods and not deleted its store it in different so it can recoveryable. So no down time needed.


# Cordon:
In Kubernetes, cordoning a node marks it as unschedulable which prevents new pods from being scheduled on it. The cordon command is used to prepare a node for maintenance or removal from a cluster.

kubectl describe node | grep -i taint
kubectl taint node master node-role.kubernetes. io/control-plane: NoSchedule-
kubectl describe node | grep -i taint

kubectl get po
kubectl get rc
kubectl get rs
kubectl get service
kubectl get all
kubect1 api-versions  # To see all apis.
for kind in 'kubectl api-resougces | tail +2 | awk '{ print $1 }''; do kubectl explain $kind; done | grep -e "KIND:" -e "VERSION:"

vim deployment.yaml|

apiVersion: apps/v1
kind: Deployment
metadata:
  name: sadhixt-deployment
  labels:
	app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

kubectl apply -f deployment.yaml  # To craete 
watch kubectl get po, deployment
kubectl get all
kubectl delete -f deployment.yaml # To delete

# Hierarchy of give pods name & IPs.
kubctl get deployment
kubctl get rs
kubctl get po

kubectl describe deployment sadhixt-deployment | less  # To see details of deployment.

# Deployment:
1) Multiple label
2) Rolling update
3) Undo -- rollback

# kubect1 rollout history #
kubctl get all
kubectl rollout history deployment.apps/sadhixt-deployment

vim Deployment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: sadhixt-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      age: nginx
  template:
    metadata:
	  labels:
	    app: nginx
  spec:
    containers:
	- name: nginx
	  image: nginx:1.7.1      # Here you can mentioned specfic no.of version and change manually.
	  ports:
      - containerPort: 80

kubectl apply -f deployment.yaml
kubectl get deployment,rs,po

kubectl create deployment webserver -- image=nginx -- replicas=2 -- dry-run=client -o yaml > webserver. yaml

kubectl set image deployment.apps/sadhixt-deployment nginx=nginx: 1.10.1 -- record=true   # To maintain Record/History.
kubectl rollout undo deployment.apps/sadhixt-deployment  # To rollback one step back means previous one.
kubectl rollout undo deployment.apps/sadhixt-deployment -- to-revsion=2  # To go exact version for rollback.

kubectl edit deployment sadhixt-deployment   # To Update or Rollback.
kubectl rollout pause deployment.apps/sadhixt-deployment   # To pause means no one can Update or Rollback.
kubectl rollout resume deployment.apps/sadhixt-deployment  # To resume pause.


# Ingress

Ingress is in below of LoadBalancer to balancing like nginx proxy & reverse proxy.
- You can give mentioned in Ingress to like goes to Lb/app, Lb/db.

================================================================================================================================================================



================================================================================================================================================================
# 28 Feb Fri - Kubernetes -11 Service

# Taint
kubectl describe node | grep -i taint
kubectl taint node master node-role.kubernetes. io/control-plane: NoSchedule-  # To disable in master node

kubectl create deployment webserver -- image=nginx -- image=ubuntu -- image=mysql -- replicas=3 -dry-run=client -o yaml  # Dry run of Deployment.

kubectl create deployment webserver -- image=nginx -- image=ubuntu -- image=mysql -- replicas=3 -dry-run=client -o yaml --show-managed-fields=false   # If true, keep the managedFields when printing objects in JSON or YAML format. And google it.

kubectl create deployment webserver -- image=nginx -r=3 --dry-run=client -o yaml  # Dry run.
kubectl create deployment webserver -- image=nginx -r=3 --dry-run=client -o yaml > web.yaml  # To save in web.yaml file.

apiVersion: apps/v1
kind: Deployment
metadata:
  Labels:
    app: web-sadhix
  name: webserver
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
        - image: nginx
          name: nginx
.

kubectl create service clusterip webserver -- tcp=80:80 -- dry-run=client service/webserver created (dry run)
kubectl create service clusterip webserver --tcp=8080:80 -- dry-run=client -o yaml
kubectl get svc
kubectl get service
kubectl delete svc

kubectl expose deployment webserver -- type="ClusterIP" -- port 8080 service/webserver exposed  # To Expose/access from outside.

## You can use whatsapp AI differents  ##

================================================================================================================================================================



================================================================================================================================================================
# 03 Mar Mon - Kubernetes -12 Service Pratical & Helm Explain

##Difference clusterip & nodeport & how to use Pending search onn google.
## RBAC - pending, google.

# Service Practical in EKS aws service
- In EKS, doesnot required handle the Master.

# To Create EKS & Its takes upto 15 mintues.
- Go to EKS then security and add ALL traffic, IP.

AWS > EKS > Create EKS Cluster
- Configuration options - new
Select first - Quick configuration (with EKS Auto Mode) - new,  Second is for custom.
- Cluster configuration
    Name - devops-testing
	Cluster IAM role = create recommended role
	Node IAM role = create recommended role
- Create  # Its takes upto 15 mintues to create.

## For access this EKS using AWScli & kubectl ##

# For kubectl in Linux.   Path:kubernetes.io/docs/tasks/tools/install-kubectl-linux/
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
mkdir -p ~/.local/bin
mv ./kubectl ~/.local/bin/kubectl
# and then append (or prepend) ~/.local/bin to $PATH

# AWS Configure
aws help
aws configure
- give Acces key ID, Access Key, Region Name, Default output format [None]: table # For keys & ID go to IAM then users.

### AWS CLI BUILDER ##
- For getting all commands.


aws eks list-clusters --region ap-south-1 --output json
aws eks describe-cluster --name devops-testing --region ap-south-1 --output json\
aws eks update-kubeconfig --name devops-testing --region ap-south-1 --output json


kubectl create deployment web --image=nginx --dry-run=client -o yaml > web.yaml
kubectl create deployment db --image=nginx --dry-run=client -o yaml > db.yaml

kubctl apply -f web.yaml
kubctl apply -f db.yaml


kubectl expose deployment db --name=db-service --type=ClusterIP -- port=8080 -- target-port=80   # To expose, so acces by using ClusterIP from outside.
kubectl exec -it web-54466ccbf7-jp92s -- /bin/bash
curl http//IP:8080  # Access from web to db.


kubectl expose deployment db --name=db-service --type=NodePort -- port=8080 --target-port=80   # Using NodePort
kubectl get svc


## Main Topics ##
1) POD
2) token
3) Deployment -- RS/RC
4) Service
5) jobs ---- create like cronjobs
8) Labels
9) Namespace
8) Labels
9) Namespace
11) certificate -- ssl
12) Helm

helm install package-name
helm --- charts

================================================================================================================================================================




================================================================================================================================================================
# 05 Mar Wed - Kubernetes -13 Helm Pratical

helmhub is a repo and hub, like github & dockerhub & YUM.


winget install Helm.Helm  # To install in windows and open again cmd prompt to use helm.

# To install in Linux.
curl fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh

# To login in EKS
helm --help
aws eks list-clusters --region ap-south-1 --output json
aws eks update-kubeconfig --name devops-testing --region ap-south-1 --output json

# To create Repo
helm create mypackage
ls
cd mypackage
- In that files are IMP, values.yaml
- In values.yaml, you can change that settings accordingly to create pods.

helm insatll mychart1 mypackage/  # Doesn't work inside that folder, so go one-step back.
helm ls
helm uninstall chart1

helm install tom mypackage/
- Then vim mypackage/values.yaml change in no.of replication.
helm upgrade tom mypackage/ # It increase the replication and add revision number to 2.

helm rollback tom 1   # To rollback to previous version it changes, but that values.yaml doest not change automatically, beacuse we are rolling back using version. So that file is unchanged.

helm install tom mypackage/ --debug --dry-run  # To see degubing mode.

# For maunally creating repos & Install
mkdir prod-app
touch prod-app/values-dev.yaml
helm install chart3 --values=prod-app/values-dev.yaml prod-app/

# Helm REPO #
helm repo ls
helm repo add bitnami https://charts.bitnami.com/bitnami   # To get repo.
helm search repo bitnami  # To see all repo list.
helm install tomtcat bitnami/tomcat  # To install.
helm ls
helm list
helm uninstall tomtcat bitnami/tomcat  # To uninstall.

================================================================================================================================================================









