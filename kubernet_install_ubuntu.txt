Note : https://jhooq.com/kubernetes-error-execution-phase-preflight-preflight/



VVIMP : https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/ 






kubeadm join 172.31.8.105:6443 --token yuisp1.jeiyk8c27uaumyqx \
        --discovery-token-unsafe-skip-ca-verification




For Ubuntu 24.04 LTS with all updates, the process is similar but with some modifications. Here’s an updated guide for setting up a multi-node Kubernetes cluster using Kubeadm on Ubuntu 24.04 LTS:



## Prerequisites
- At least two Ubuntu 24.04 LTS servers with 2GB RAM and 2 CPU cores each  t2.micro ( 2nd one) 
- Network connectivity between servers --- ping -- private ip ( ping ) 
- Root access to each server ---- 



Update the system and install dependencies:


sudo apt update && sudo apt upgrade -y
sudo apt install apt-transport-https curl -y


Install containerd:

sudo apt install containerd -y


Configure containerd:

sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml > /dev/null
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
sudo systemctl restart containerd


Disable swap:

sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab


Load necessary kernel modules:

sudo modprobe overlay
sudo modprobe br_netfilter


# Set required sysctl parameters:


cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
...

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
...
sudo sysctl --system


Install Kubernetes components:

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

--------------------------------------------------------------------------------------------- Till this we have to run all node that in cluster

--------------After this we have to run on master node ( control-plan)  -------- 

Initialize the cluster (run only on master node):

sudo kubeadm init --pod-network-cidr=10.244.0.0/16  ( dont change this ip ) ---- this is for flannel netwrok plugin   ### ( run on master node only ) 


Set up kubeconfig for the user:


mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

KUBECONFIG=$HOME/.kube/config



Install Flannel network plugin (run only on master node):

 wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

 kubectl apply -f kube-flannel.yml


Verify the installation:

kubectl get nodes
kubectl get pods --all-namespaces

#################################
##copilot
But Wait — One More Thing for Single-Node Use
By default, Kubernetes taints the control plane node to prevent scheduling regular pods on it. Since you don’t have worker nodes, you’ll want to remove that taint:

kubectl taint nodes --all node-role.kubernetes.io/control-plane-
kubectl taint nodes <node-name> node-role.kubernetes.io/control-plane=:NoSchedule
#################################


Join worker nodes to the cluster:

Use the kubeadm join command provided by the kubeadm init output on the master node.
Run this command with sudo on each worker node.
kubeadm join 172.31.19.36:6443 --token 922x9d.v0jn4c8he0s286js --discovery-token-ca-cert-hash sha256:8897fd8eb97f2ea0686ccf7507f287ffffd5cf681496fb324940330561c80e4c

### kubeadm token create --print-join-command ## On master to get join command. 


for troubleshooting if your join command will not work 



-- security enable 6443 port to work join command 
-
sudo systemctl restart containerd
sudo systemctl restart kubelet
reboot your machine (may be optional)

====== Incase you are using docker as cri follow theses command open vi /etc/docker/daemon.json change this ["native.cgroupdriver=systemd"] to "exec-opts": ["native.cgroupdriver=cgroupfs"],  ----- (may be optional)


sudo systemctl daemon-reload
sudo systemctl restart kubelet




kubectl create deployment nginx --image=nginx
kubectl expose deployment nginx --type=NodePort --port=80
kubectl get svc nginx



====================================================================================
# Kubernetes Master Setup on Ubuntu 22.04+ (All-in-One)

# 1. Set hostname (optional)
sudo hostnamectl set-hostname master-node

# 2. Disable swap
sudo swapoff -a
sudo sed -i '/swap/d' /etc/fstab

# 3. Load kernel modules
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
sudo modprobe br_netfilter

# 4. Set sysctl params
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables=1
net.ipv4.ip_forward=1
net.bridge.bridge-nf-call-ip6tables=1
EOF
sudo sysctl --system

# 5. Install containerd
sudo apt update && sudo apt install -y containerd

# 6. Configure containerd with systemd cgroup driver
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml > /dev/null
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
sudo systemctl restart containerd
sudo systemctl enable containerd

# 7. Add Kubernetes repo
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update

# 8. Install Kubernetes core tools
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
sudo systemctl enable --now kubelet

# 9. Initialize the cluster (using Flannel network CIDR)
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

# 10. Set up kubectl config
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 11. Deploy Flannel CNI
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
====================================================================================

====================================================================================
# Full Cleanup on Ubuntu Worker Nod

# 1. Reset the cluster state
sudo kubeadm reset -f

# 2. Stop services
sudo systemctl stop kubelet
sudo systemctl stop containerd

sudo rm -rf ~/.kube
sudo rm -rf /etc/kubernetes
sudo rm -rf /var/lib/etcd
sudo rm -rf /var/lib/kubelet
sudo rm -rf /etc/cni/net.d
sudo rm -rf /var/lib/cni
sudo rm -rf /opt/cni
sudo ip link delete cni0    2>/dev/null
sudo ip link delete flannel.1 2>/dev/null

rm -rf $HOME/.kube
rm -f $HOME/.kube/config

sudo apt remove --purge -y kubeadm kubelet kubectl
sudo apt autoremove -y
sudo apt remove --purge -y containerd
sudo rm -rf /etc/containerd

sudo sed -i '/master-node/d' /etc/hosts

which kubeadm
which kubelet
which kubectl
ls /etc/kubernetes
===============================================================================

====================================================================================
# CentOS Kubernetes Node Setup (All Commands in One Go)

# Set hostname (optional)
sudo hostnamectl set-hostname worker-node

# Disable swap & SELinux
sudo swapoff -a
sudo sed -i '/swap/d' /etc/fstab
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing/SELINUX=permissive/' /etc/selinux/config

# Load kernel modules
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

# Set sysctl parameters
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
sudo sysctl --system

# Install containerd
sudo dnf install -y yum-utils device-mapper-persistent-data lvm2
sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo dnf install -y containerd.io

# Generate default containerd config and set cgroup driver
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml > /dev/null
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

# Start containerd
sudo systemctl daemon-reexec
sudo systemctl enable --now containerd

# Add Kubernetes repo
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.30/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.30/rpm/repodata/repomd.xml.key
EOF

# Install Kubernetes tools
sudo dnf install -y kubelet kubeadm kubectl
sudo systemctl enable --now kubelet
====================================================================================

====================================================================================
# Full Cleanup on CentOS Worker Nod

# 1. Reset kubeadm state
sudo kubeadm reset -f

# 2. Stop services
sudo systemctl stop kubelet
sudo systemctl stop containerd

# 3. Clean up Kubernetes & CNI files
sudo rm -rf /etc/kubernetes
sudo rm -rf /var/lib/kubelet
sudo rm -rf /etc/cni/net.d
sudo rm -rf /var/lib/cni
sudo rm -rf ~/.kube

rm -rf $HOME/.kube
rm -f $HOME/.kube/config

# 4. Remove network interfaces (ignore errors if not present)
sudo ip link delete cni0 2>/dev/null
sudo ip link delete flannel.1 2>/dev/null

# 5. Free any occupied Kubernetes-related ports
sudo lsof -ti :10250 | xargs -r sudo kill -9

# 6. Restart core services
sudo systemctl restart containerd
sudo systemctl restart kubelet

# 7. Verify clean state
which kubeadm
which kubelet
which kubectl
ls /etc/kubernetes
==================================================================================	



kubeadm join 192.168.1.12:6443 --token j31tfq.a6utjepvr19egl8r \--discovery-token-ca-cert-hash sha256:ee31627c9572d22feff77cd01a188961fba82972124a85d9a197e15fab039dec


kubectl delete all --all
kubectl delete ingress --all
kubectl delete configmap --all
kubectl delete pvc --all

kubectl delete ns haproxy-controller

------------------------
# Problem #
sed -i '/swap.img/d' /etc/fstab  # Optional: prevent swap from re-enabling on reboot
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"

# Solution #
kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes
export KUBECONFIG=/etc/kubernetes/admin.conf
grep certificate-authority /etc/kubernetes/admin.conf
ls -l /etc/kubernetes/pki/ca.crt
openssl x509 -in /etc/kubernetes/pki/ca.crt -text -noout
kubectl get node
## kubeadm init --pod-network-cidr=10.244.0.0/16  ## Only when above commands not working or not getting solutions from above commands.

# Permenent Solution #
mkdir -p ~/.kube
cp /etc/kubernetes/admin.conf ~/.kube/config

# Add below line in vi ~/.bashrc OR ~/.profile then source ~/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf

ls -l /etc/kubernetes/pki/ca.crt
openssl x509 -in /etc/kubernetes/pki/ca.crt -text -noout | grep Issuer
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep Issuer

## If above commands not working then use below cmds otherwise ignore.
kubeadm reset
kubeadm init --pod-network-cidr=10.244.0.0/16

---------------------------


OK thnx. Now I telling whole story.
Its basically loan santioned is 40 Lakh.
For 10 years and Interest is 8.60% till now.

But In june 2023 loan starts and amount for 25 lakh only, this is first installment.
Reamining 15 lakh I dont know right now.

So basically we completed 2years with 8.60% Interest rate with monthly EMI is 31,000.
Now its showing 22 lakh pending for 8 years.

If I pay 5lakh now, then pending amount is 17 lakh. Current Interest Rate is 8.1%
1. Then my EMI amount is same, but I think amount goes to more principal side.
2. If I additionally pay extra 10,000 rs

can you provide me, interactive excel, so I can add extra amount and see. example suppose I pay extra amount in any months. So i can understand this things. You got it ??



=========================================

# Phase 1

 oc get nodes
 oc get pvc
 oc get pv
 oc get nodes
 oc cluster-info
 oc get namespaces
 oc get pods -n kube-system
 oc get node
 oc describe node k8s
 oc get pods -A
 oc get componentstatuses
 oc config get-contexts
 oc get nodes
 oc get pods -n kube-system
 oc get pods -A

# Phase 2

 mkdir phase2
 cd phase2/
 ll
 oc run nginx -image=nginx
 oc run nginx --image=nginx
 oc get pods
 oc create deployment webapp --image=nginx
 oc get deplyoments
 oc get pods
 oc scale deployment webapp --replicas=3
 oc get pods
 oc expose deployment webapp --port=80 --type=NodePort
http://<your-VM-IP>:<NodePort>
e.g., http://192.168.1.100:30596

 oc get svc
 ip r l
 oc delete deployment webapp
 oc delete service webapp
 oc delete pod nginx
 
# Phase 3

 oc create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
 cat nginx-deployment.yaml
 vi nginx-deployment.yaml
 oc apply -f nginx-deployment.yaml
 oc get deployments
 oc get pods
 vi nginx-service.yaml
 ll
 cat nginx-deployment.yaml
 oc aplly -f nginx-service.yaml
 oc apply -f nginx-service.yaml
 oc get svc
 oc delete -f nginx-deployment.yaml
 oc delete -f nginx-service.yaml
 
# Phase 4

sudo mkdir -p /data/test
sudo chmod 777 /data/test/
vi hostpath-pod.yaml
oc apply -f hostpath-pod.yaml
oc get pods
echo "Hello from Aniket HostPath Volume" | sudo tee /data/test/index.html
oc exec -it nginx-volume-pod -- cat /usr/share/nginx/html/index.html
oc label pod nginx-volume-pod app=nginx
oc expose pod nginx-volume-pod --port=80 --type=NodePort  # Without label not possible to expose.
oc get svc
oc delete -f hostpath-pod.yaml
oc delete svc nginx-volume-pod
##oc delete label nginx-volume-pod


# Phase 4.2: Persistent Volume (PV) + Persistent Volume Claim (PVC)

mkdir phase4.2
cd phase4.2/
vi pv.yaml
vi pvc.yaml
oc apply -f pv.yaml
oc apply -f pvc.yaml
oc get pv
oc get pvc
vi pvc-pod.yaml
oc apply -f pvc-pod.yaml
oc get pods
sudo mkdir -p /data/pv
echo "Hello from PVC!" | sudo tee /data/pv/index.html
oc exec -it nginx-pvc-pod -- cat /usr/share/nginx/html/index.html
oc expose pod nginx-pvc-pod --port=80 --type=NodePort
oc get svc
oc delete pod nginx-pvc-pod
oc delete svc nginx-pvc-pod
oc delete pvc my-pvc
oc delete pv my-pv

#oc delete -f pvc-pod.yaml
#oc delete -f pvc.yaml
#oc delete -f pv.yaml
#oc delete svc nginx-pvc-pod


# Phase 5: ConfigMaps & Secrets in Kubernetes

oc create configmap site-config --from-literal=site_name="Appice Kubernetes"
oc get configmap site-config -o yaml
oc create secret generic db-secret --from-literal=db_pass='AniSecure123'
oc get secret db-secret -o yaml
echo QW5pU2VjdXJlMTIz | base64 --decode
vi configmap-secret-pod.yaml
oc apply -f configmap-secret-pod.yaml
oc get pods
oc exec -it env-demo -- env | grep -E 'SITE|DB_'
oc delete pod env-demo
oc delete configmap site-config
oc delete secret db-secret

#rm configmap-secret-pod.yaml
#oc delete pod env-demo
#oc delete configmap site-config
#oc delete secret db-secret


# Phase 6: Kubernetes Networking & Ingress

oc get pods -n kube-system | grep dns
coredns-xxxxx Running
no
pwd
vi webapp1.yaml
vi webapp2.yaml
oc apply -f webapp1.yaml
oc apply -f webapp2.yaml
sudo kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.4/deploy/static/provider/baremetal/deploy.yaml
oc get pods -n ingress-nginx
vi ingress-rule.yaml
oc get pods -n ingress-nginx
oc apply -f ingress-rule.yaml
oc get svc -n ingress-nginx
oc get pods -n ingress-nginx
sudo kubectl get svc -n ingress-nginx

# In browser
http://192.168.1.12:32514/app1
http://192.168.1.12:31251/app2

# In Command Line
curl -H "Host: hello.local" http://192.168.1.12:32514/
curl -H "Host: hello.local" http://192.168.1.12:31251/

oc delete -f webapp1.yaml
oc delete -f webapp2.yaml
oc delete -f ingress-rule.yaml

#oc delete -f webapp1.yaml
#oc delete -f webapp2.yaml
#oc delete -f ingress-rule.yaml
#rm webapp1.yaml webapp2.yaml ingress-rule.yaml
 

# Phase 7: RBAC & Namespaces in Kubernetes

oc create namespace team-a
oc get ns

vi pod-reader-role.yaml
oc apply -f pod-reader-role.yaml

oc create serviceaccount reader-user -n team-a
oc get serviceaccounts -n team-a

vi pod-reader-binding.yaml
oc apply -f pod-reader-binding.yaml

## PENDING ###############
Step 5: Use This Account via Token (optional advanced test)

oc run test-reader \
  --image=busybox \
  --restart=Never \
  --serviceaccount=reader-user \
  --namespace=team-a \
  --command -- sleep 3600

oc exec -n team-a -it test-reader -- sh

# Should work
kubectl get pods -n team-a

# Should fail
kubectl delete pod test-reader

###################
oc delete -f pod-reader-binding.yaml
oc delete -f pod-reader-role.yaml
oc delete serviceaccount reader-user -n team-a
oc delete pod test-reader -n team-a
oc delete namespace team-a

# Plus all soft cleanup
#rm pod-reader-role.yaml pod-reader-binding.yaml


# Phase 8: Probes & Auto-scaling

vi probe-demo.yaml
apiVersion: v1
kind: Pod
metadata:
  name: probe-demo
  labels:
    app: probe-demo
spec:
  containers:
  - name: demo
    image: nginx
    ports:
    - containerPort: 80
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 10
    readinessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 3
      periodSeconds: 5

oc apply -f probe-demo.yaml
oc describe pod probe-demo

📦 Auto-scaling (HPA)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

oc create deployment cpu-app --image=nginx
oc expose deployment cpu-app --port=80

oc autoscale deployment cpu-app --cpu-percent=50 --min=1 --max=5

oc get hpa

Phase 8 Cleanup
#oc delete -f probe-demo.yaml
oc delete deployment cpu-app
oc delete svc cpu-app
oc delete hpa cpu-app


# Phase 9: Jobs & CronJobs

vi job-example.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-job
spec:
  template:
    spec:
      containers:
      - name: hello
        image: busybox
        command: ["echo", "Hello from Kubernetes Job"]
      restartPolicy: Never


oc apply -f job-example.yaml

oc get jobs
oc logs job/hello-job

🕒 CronJob Example
vi cronjob-example.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello-cron
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cron
            image: busybox
            command: ["echo", "Hello every minute"]
          restartPolicy: OnFailure

oc apply -f cronjob-example.yaml

oc get cronjobs
oc get jobs

# Wait 1–2 mins, then check logs:
oc logs job/<generated-job-name>

🧹 Phase 9 Cleanup
oc delete -f job-example.yaml
oc delete -f cronjob-example.yaml


# 📦 Phase 10: Helm — Kubernetes Package Manager

sudo curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
helm version

helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

helm install my-nginx bitnami/nginx

oc get all

helm get values my-nginx

helm upgrade my-nginx bitnami/nginx --set service.type=NodePort

helm uninstall my-nginx


7756937901
Meera@19752000

Aslot

slot

slot

<select name="ct100$ContentPlaceHolder1$UCPpolic
eStationAddress $ddl_LPS" id="ContentPlaceHolder
1_UCPoliceStationAddress_ddl_LPS">
<option value> ---- Select Police Stn ----
</option>
<option value="61">Bhiwandi Tal P. Station
</option>
<option value="60">Ganeshpuri P. Station
</option>
<option value="67">Kalyan Tal/Titwala P.
Station</option>
<option value="65">Kasara P. Station</option>
slot
<option value="66">Kinhavli P. Station
</option> slot
<option value="69">Kulgaon P. Station
slot
</option>
<option value="68">Murbad P. Station</option>
slot
<option value="64">Padgha P. Station</option>
slot
<option value="62">Shahapur P. Station
</option> slot
<option value="70">Tokwade P. Station
</option>
<option value="63">Vasind P. Station</option>
slot

In that - "Ulhasnagar P. Station" is missing.

https://learn.kodekloud.com/courses/udemy-labs-online-kubernetes-lab-for-beginners-hands-on